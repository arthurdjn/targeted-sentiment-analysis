{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Set Up**\n",
    "\n",
    "Before diving in the models, let's reload the notebook to keep it updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's load the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, PackedSequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data science\n",
    "import spacy\n",
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's not forget to fix the seed for random generated numbers !\n",
    "SEED = 2020 \n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Baseline**\n",
    "\n",
    "\n",
    "# 1. Dataset\n",
    "\n",
    "### 1.1. NoReC fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import SequenceTaggingDataset\n",
    "\n",
    "\n",
    "class NoReCfine(SequenceTaggingDataset):\n",
    "    def __init__(self, path, fields, encoding=\"utf-8\", separator=\"\\t\", **kwargs):\n",
    "        super().__init__(path, fields)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, fields, train_data=\"data/train.conll\", dev_data=\"data/dev.conll\", test_data=\"data/test.conll\"):\n",
    "        return NoReCfine(train_data, fields), NoReCfine(dev_data, fields), NoReCfine(test_data, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "TEXT = torchtext.data.Field(lower=False, include_lengths=True, batch_first=True)\n",
    "LABEL = torchtext.data.Field(batch_first=True, unk_token=None)\n",
    "FIELDS = [(\"text\", TEXT), (\"label\", LABEL)]\n",
    "\n",
    "train_data, eval_data, test_data = NoReCfine.splits(FIELDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 5,915\n",
      "Number of training examples: 1,151\n",
      "Number of testing examples:    895\n",
      "\n",
      "Number of sentences in train_data.text: 5915\n",
      "Number of words in train_data: 98,483\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data):,}')\n",
    "print(f'Number of training examples: {len(eval_data):,}')\n",
    "print(f'Number of testing examples:    {len(test_data)}')\n",
    "\n",
    "text_length = [len(sentence) for sentence in list(train_data.text)]\n",
    "\n",
    "print(f\"\\nNumber of sentences in train_data.text: {len(text_length)}\")\n",
    "print(f'Number of words in train_data: {sum(text_length):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's inside the training data:\n",
      "{'text': ['Lite', 'tight', 'Tempah'], 'label': ['O', 'O', 'B-targ-Negative']}\n"
     ]
    }
   ],
   "source": [
    "print(\"What's inside the training data:\")\n",
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "\n",
    "VOCAB_SIZE = 1_200_000\n",
    "VECTORS = Vectors(name='model.txt')\n",
    "\n",
    "# Create the vocabulary for words embeddings\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = VOCAB_SIZE, \n",
    "                 vectors = VECTORS, \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1182371, 100])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTORS.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19192, 100])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary defined on the training data, with the help of pre-trained embeddings:\n",
      "\n",
      "['<unk>', '<pad>', '.', ',', 'og', 'er', 'i', 'som', 'en', 'det', 'på', 'å', 'av', 'med', 'til', 'for', '«', '»', 'har', 'den']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary defined on the training data, with the help of pre-trained embeddings:\\n\")\n",
    "\n",
    "print(TEXT.vocab.itos[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'Martel' is not part of the pre-trained embeddings.\n",
      "\n",
      "PyTorch sets its vector to zero:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(f\"The word '{TEXT.vocab.itos[8867]}' is not part of the pre-trained embeddings.\\n\")\n",
    "\n",
    "print(f\"PyTorch sets its vector to zero:\\n{TEXT.vocab.vectors[8867]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary informations:\n",
      "\n",
      "Labels vocabulary:\n",
      "['<pad>', 'O', 'I-targ-Positive', 'B-targ-Positive', 'I-targ-Negative', 'B-targ-Negative']\n",
      "\n",
      "Text vocabulary:\n",
      "['<unk>', '<pad>', '.', ',', 'og', 'er', 'i', 'som', 'en', 'det']\n",
      "\n",
      "Most frequent words:\n",
      "[('.', 4585), (',', 4085), ('og', 2878), ('er', 2326), ('i', 2071), ('som', 1699), ('en', 1628), ('på', 1318), ('det', 1318), ('å', 1222)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary informations:\\n\")\n",
    "print(f\"Labels vocabulary:\\n{LABEL.vocab.itos}\\n\")\n",
    "print(f\"Text vocabulary:\\n{TEXT.vocab.itos[:10]}\\n\")\n",
    "print(f\"Most frequent words:\\n{TEXT.vocab.freqs.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
