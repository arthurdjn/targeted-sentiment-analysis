; This configuration file is used to set limits to the Grid Search optimization
; The configuration is divided into five parts, Options, Hyper, Model, Criterion and Optimizer, which
; affects respectively their inner parameters.
; The options part is used for general information, like saving directory etc.


; Some general options, that do not affect the algorithm.
[Options]
; If True, will save models and checkpoints for all searches.
; @type: bool
saves = True
; Path to the saving directory.
; @type: string
dirsaves = "saves"
; If True, will display a log with statistics.
; @type: bool
verbose = True
; Compare models on which scores
; @type: string
compare_on = "f1_score"


; IMPORTANT !
; Note that all parameters values are contained in a list.
; The reason is that you can add multiples values to search in (for example, let's search within a set of batch sizes
; batch_size = [32, 64, 128].

; IMPORTANT !
; If you want to use specific modules or packages, you don't need to initialize them here.
; The Tuner class will handle it for you, as the initialization may vary during the grid search.
; DO's : torch.nn.CrossEntropy
; DONT's : torch.nn.CrossEntropy()


; HyperParameters tuning.
[Hyper]
; The batch size is used to divided the training data (and eval, test as well) into samples of size batch_size.
; Note that all data iterator will be affected the same way (train, eval, test).
; @type: int
batch_size = [64]
; The number of epochs is the number of loops the model will run.
; @type: int
epochs = [100]
; The learning rate is how much the weights are change.
; @type: float
lr = [0.05, 0.1, 0.2]
; Word embeddings vectors file
; @type: string
vectors_name = 'model.txt'
; If the vectors are not saved locally, download them from:
; @type: string
vectors_url = 'http://vectors.nlpl.eu/repository/20/58.zip'
; If True, use pretrained words embeddings from ``vectors_name`` as initial state.
; @type: bool
use_pretrained_embeddings = True


; Models parameters and attributes tuning.
; This section is a "template", and you are welcome to add or modify it.
; For more informations, check the online documentation here :
; https://pages.github.uio.no/arthurd/in5550-exam/source/package.html#module-sentarget.optim.tuner

; NOTE : you can add more arguments that your models needs, they will just not be used (and won't crash the algo).

; The default notation, to respect is to use double underscore '__' to specify that you want to modify
; some parameters corresponding to a model's attribute, or to use a dot '.' to change an attribute from a
; module that is not saved as an attribute. (To check all your model's modules, simply use : ``model.modules()``.
; This notation comes from sklearn and skorch.
; Example :
; linear1__in_features = [10, 20, 30, 40] will change the ``in_features`` attribute for the model's attribute ``linear1``,
; which is in this case a ``Linear`` module. The values will vary from 10 to 40.
; Example :
; Linear.in_features = [10, 20, 30, 40] will modify the ``in_features`` attribute for all ``Linear`` modules
; stocked in your model.
[Model]
; Models type to use
; @type: sentarget.models.model.Model
model = [sentarget.nn.models.lstm.BiLSTM, sentarget.nn.models.gru.BiGRU]
; The input dimension should not be modified (depending on what you want).
; Should be equals to len(TEXT.vocab)
; @type: int
input_dim = [23574]
; The embedding_dim dimension should not be modified (depending on what you want).
; @type: int
embedding_dim = [100]
; Modify it !
; @type: int
hidden_dim = [150, 200, 250]
; The output_dim dimension should not be modified (depending on what you want).
; Should be equals to len(LABEL.vocab)
; @type: int
output_dim = [6]
; Modify it !
; @type: int
n_layers = [2, 3]
; Modify it !
; @type: bool
bidirectional = [True]
; Modify it ! Be careful with dropout = 0 and n_layers = 1.
; @type: float
dropout = [0.1, 0.2, 0.3]
; The pad_idx argument should not be modified (depending on what you want).
; Should be the same as TEXT.vocab.stoi[TEXT.pad_token]
; @type: int
pad_idx_text = [1]
; The unk_idx argument should not be modified (depending on what you want).
; Should be the same as TEXT.vocab.stoi[TEXT.unk_token]
; @type: int
unk_idx_text = [0]
; The pad_idx argument should not be modified (depending on what you want).
; Should be the same as LABEL.vocab.stoi[LABEL.pad_token]
; @type: int
pad_idx_label = [0]


[Criterion]
; Ignore indices when the loss is used.
; Should be the same as LABEL.vocab.stoi[LABEL.pad_token]
; @type: int
ignore_index = 0
; The criterion is another name for the loss function to use.
; @type: Loss
criterion = [torch.nn.CrossEntropyLoss]
; Custom weights for all classes
; @type: torch.Tensor
weight = [torch.tensor([1, 0.06771941, 0.97660534, 0.97719714, 0.98922782, 0.98925029])]


; Some parameters for the optimizer.
[Optimizer]
; The optimizer is the weights' update rule.
; @type: Optimizer
optimizer = [torch.optim.Adam]
